# åˆ†å­åŠ¨åŠ›å­¦è‡ªç”±èƒ½è®¡ç®—é¡¹ç›® - ä»£ç å¯¼è¯»æŒ‡å—

> ğŸ§­ **é¡¹ç›®å¯¼è¯»** - æ·±å…¥ç†è§£ä»£ç æ¶æ„ã€æ•°æ®æµå’Œæ ¸å¿ƒç®—æ³•

## ğŸ“– é˜…è¯»æŒ‡å—

å¦‚æœä½ æ˜¯ç¬¬ä¸€æ¬¡æ¥è§¦è¿™ä¸ªé¡¹ç›®ï¼Œå»ºè®®æŒ‰ä»¥ä¸‹é¡ºåºé˜…è¯»ä»£ç ï¼š

### ğŸ”° åˆå­¦è€…è·¯å¾„
1. **é…ç½®ç³»ç»Ÿ** (`config.py`) - äº†è§£é¡¹ç›®å‚æ•°ç®¡ç†
2. **ä¸»è„šæœ¬** (`main_train.py`) - ç†è§£é¡¹ç›®å…¥å£å’Œæµç¨‹
3. **æ•°æ®å¤„ç†** (`util/test_lambda_emb_dataset.py`) - ç†è§£æ•°æ®æ ¼å¼å’Œé¢„å¤„ç†
4. **æ¨¡å‹å®šä¹‰** (`models/encoder_cnn_model.py`) - ç†è§£æ ¸å¿ƒç®—æ³•

### ğŸ”¬ ç ”ç©¶è€…è·¯å¾„
1. **æŸå¤±å‡½æ•°** (`util/enc_model_dg_loss.py`) - ç†è§£ç‰©ç†çº¦æŸ
2. **è®­ç»ƒå¼•æ“** (`util/enc_engine_*.py`) - ç†è§£è®­ç»ƒç­–ç•¥
3. **æ¨¡å‹ç»†èŠ‚** (`models/encoder_cnn_model.py`) - æ·±å…¥ç®—æ³•å®ç°
4. **è®­ç»ƒå™¨** (`trainer.py`) - ç†è§£å®Œæ•´è®­ç»ƒæµç¨‹

### ğŸ”§ å·¥ç¨‹å¸ˆè·¯å¾„
1. **æ¶æ„è®¾è®¡** (æœ¬æ–‡æ¡£) - äº†è§£æ•´ä½“è®¾è®¡æ€è·¯
2. **é…ç½®ç³»ç»Ÿ** (`config.py`) - ç†è§£å‚æ•°ç®¡ç†
3. **è®­ç»ƒå™¨** (`trainer.py`) - ç†è§£å·¥ç¨‹å®ç°
4. **æ¨¡å—äº¤äº’** (å„æ¨¡å—) - ç†è§£æ¥å£è®¾è®¡

---

## ğŸ—ï¸ é¡¹ç›®æ¶æ„æ€»è§ˆ

### æ ¸å¿ƒè®¾è®¡ç†å¿µ

```
ğŸ¯ ç›®æ ‡ï¼šä½¿ç”¨æ·±åº¦å­¦ä¹ é¢„æµ‹åˆ†å­ç³»ç»Ÿçš„è‡ªç”±èƒ½å˜åŒ–(Î”G)
ğŸ“Š æ•°æ®ï¼šÎ»çª—å£é‡‡æ ·çš„åˆ†å­åŠ¨åŠ›å­¦æ—¶é—´åºåˆ—
ğŸ§  æ¨¡å‹ï¼šåŸºäºTransformerçš„è‡ªç¼–ç å™¨æ¶æ„
ğŸ”¬ ç‰©ç†ï¼šç»“åˆåˆ†å­åŠ¨åŠ›å­¦çŸ¥è¯†çš„æŸå¤±å‡½æ•°è®¾è®¡
```

### æ¶æ„å›¾

```mermaid
graph TB
    subgraph "æ•°æ®å±‚"
        CSV[åŸå§‹CSVæ•°æ®] --> Dataset[CustomDataset]
        Dataset --> Processor[LambdaDataProcessor]
        Processor --> DataLoader[DataLoader]
    end
    
    subgraph "é…ç½®å±‚"
        Args[å‘½ä»¤è¡Œå‚æ•°] --> Config[é…ç½®ç³»ç»Ÿ]
        Config --> Trainer[è®­ç»ƒå™¨]
    end
    
    subgraph "æ¨¡å‹å±‚"
        DataLoader --> Model[MaskedAutoencoderViT]
        Model --> Embedding[åµŒå…¥ç­–ç•¥]
        Embedding --> Transformer[Transformerç¼–ç å™¨]
        Transformer --> Projection[æŠ•å½±å¤´]
    end
    
    subgraph "è®­ç»ƒå±‚"
        Projection --> Loss[å¤šæŸå¤±å‡½æ•°]
        Loss --> Optimizer[ä¼˜åŒ–å™¨]
        Optimizer --> Trainer
    end
    
    subgraph "è¾“å‡ºå±‚"
        Trainer --> TensorBoard[TensorBoardæ—¥å¿—]
        Trainer --> Checkpoint[æ¨¡å‹æ£€æŸ¥ç‚¹]
        Trainer --> Results[éªŒè¯ç»“æœ]
    end
```

---

## ğŸ“‚ æ¨¡å—è¯¦ç»†è§£æ

### 1. é…ç½®ç®¡ç†ç³»ç»Ÿ (`config.py`)

#### ğŸ¯ æ ¸å¿ƒèŒè´£
ç»Ÿä¸€ç®¡ç†é¡¹ç›®çš„æ‰€æœ‰é…ç½®å‚æ•°ï¼Œæä¾›ç±»å‹å®‰å…¨çš„é…ç½®è®¿é—®ã€‚

#### ğŸ”‘ å…³é”®ç»„ä»¶

```python
@dataclass
class Config:
    data: DataConfig      # æ•°æ®ç›¸å…³é…ç½®
    model: ModelConfig    # æ¨¡å‹ç›¸å…³é…ç½®  
    training: TrainingConfig  # è®­ç»ƒç›¸å…³é…ç½®
    output: OutputConfig  # è¾“å‡ºç›¸å…³é…ç½®
```

#### ğŸŒŸ è®¾è®¡äº®ç‚¹
- **ç±»å‹å®‰å…¨**: ä½¿ç”¨dataclassæä¾›è‡ªåŠ¨ç±»å‹æ£€æŸ¥
- **æ™ºèƒ½è®¡ç®—**: è‡ªåŠ¨è®¡ç®—æœ‰æ•ˆæ‰¹æ¬¡å¤§å°å¯¹åº”çš„å­¦ä¹ ç‡
- **éªŒè¯æœºåˆ¶**: é…ç½®æœ‰æ•ˆæ€§æ£€æŸ¥å’Œè®¾å¤‡å¯ç”¨æ€§æ£€æµ‹
- **çµæ´»åˆ›å»º**: æ”¯æŒä»å‘½ä»¤è¡Œå‚æ•°æˆ–ä»£ç ç›´æ¥åˆ›å»º

#### ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹
```python
# ä»å‘½ä»¤è¡Œåˆ›å»ºé…ç½®
config = Config.from_args(args)
config.validate()
config.print_config()

# è®¿é—®é…ç½®
batch_size = config.training.batch_size
model_name = config.model.model_name
```

---

### 2. æ•°æ®å¤„ç†æ¨¡å— (`util/test_lambda_emb_dataset.py`)

#### ğŸ¯ æ ¸å¿ƒèŒè´£
å¤„ç†åˆ†å­åŠ¨åŠ›å­¦åŸå§‹æ•°æ®ï¼Œè½¬æ¢ä¸ºæ·±åº¦å­¦ä¹ æ¨¡å‹å¯ç”¨çš„æ ¼å¼ã€‚

#### ğŸ”‘ å…³é”®ç»„ä»¶

##### `LambdaDataProcessor` ç±»
**åŠŸèƒ½**: æ•°æ®æ ‡å‡†åŒ–å’Œå¯¹é½
```python
åŸå§‹æ•°æ®: å¯å˜é•¿åº¦Î»çª—å£ â†’ æ ‡å‡†åŒ–: 100çª—å£å›ºå®šç½‘æ ¼
è¾“å…¥: [Nä¸ªæ ·æœ¬, æ¯æ ·æœ¬Mä¸ªçª—å£, æ¯çª—å£Kä¸ªæ•°æ®ç‚¹]
è¾“å‡º: [N, C, 100, max_data] å¼ é‡ + æ©ç ä¿¡æ¯
```

**å…³é”®ç‰¹æ€§**:
- **çª—å£å¯¹é½**: å°†ä¸åŒÎ»å€¼çš„çª—å£æ˜ å°„åˆ°æ ‡å‡†0.01é—´éš”ç½‘æ ¼
- **é•¿åº¦ç»Ÿä¸€**: å¡«å……æˆ–æˆªæ–­åˆ°å›ºå®šé•¿åº¦
- **æ©ç ç”Ÿæˆ**: è®°å½•æœ‰æ•ˆçª—å£å’ŒåŸå§‹æ•°æ®é•¿åº¦
- **å¤šé€šé“æ”¯æŒ**: çµæ´»å¤„ç†3é€šé“æˆ–4é€šé“æ•°æ®

##### `CustomDataset` ç±»
**åŠŸèƒ½**: æ•°æ®é›†åŠ è½½å’Œç®¡ç†
```python
æ–‡ä»¶ç»“æ„: system_X/complex|ligand/*.csv + fe_cal_out/free_ene_zwanzig.csv
è§£æè§„åˆ™: *_lambda{Î»å€¼}_delta{Î´Î»å€¼}.csv
```

**å…³é”®ç‰¹æ€§**:
- **æ™ºèƒ½è§£æ**: è‡ªåŠ¨ä»æ–‡ä»¶åæå–Î»å’ŒÎ”Î»å€¼
- **éšæœºé‡‡æ ·**: æ¯ä¸ªç³»ç»Ÿç”Ÿæˆå¤šä¸ªéšæœºå­é›†è¿›è¡Œæ•°æ®å¢å¼º
- **ç›®æ ‡åŠ è½½**: ä»free_ene.csvåŠ è½½çœŸå®Î”Gå€¼
- **æ‰¹å¤„ç†**: é€šè¿‡custom_collate_fnå¤„ç†å¯å˜é•¿åº¦æ•°æ®

#### ğŸŒŠ æ•°æ®æµå‘
```
1. CSVæ–‡ä»¶è¯»å– â†’ pandas DataFrame
2. æ–‡ä»¶åè§£æ â†’ Î», Î”Î» å‚æ•°æå–
3. éšæœºå­é›†é‡‡æ · â†’ æ•°æ®å¢å¼º
4. LambdaDataProcessor â†’ æ ‡å‡†åŒ–å¯¹é½
5. DataLoader + collate_fn â†’ æ‰¹æ¬¡ç»„ç»‡
```

#### ğŸ’¡ æ•°æ®æ ¼å¼ç¤ºä¾‹
```python
# è¾“å…¥æ•°æ®
processed_data_dict = {
    'data': torch.Tensor,           # [N, C, 100, 50] æ ‡å‡†åŒ–æ•°æ®
    'lambdas': torch.Tensor,        # [N, 100] Î»å€¼
    'deltas': torch.Tensor,         # [N, 100] Î”Î»å€¼  
    'masks': {
        'window': torch.Tensor,     # [N, 100] æœ‰æ•ˆçª—å£æ©ç 
        'delta': torch.Tensor       # [N, 100] çœŸå®Î”Î»æ©ç 
    },
    'original_lengths': torch.Tensor # [N, 100] åŸå§‹æ•°æ®é•¿åº¦
}
```

---

### 3. æ¨¡å‹å®šä¹‰æ¨¡å— (`models/encoder_cnn_model.py`)

#### ğŸ¯ æ ¸å¿ƒèŒè´£
å®ç°åŸºäºTransformerçš„è‡ªç”±èƒ½é¢„æµ‹æ¨¡å‹ï¼Œç»“åˆç‰©ç†çŸ¥è¯†å’Œæ·±åº¦å­¦ä¹ ã€‚

#### ğŸ§© æ¨¡å‹æ¶æ„

```mermaid
graph LR
    Input[è¾“å…¥æ•°æ®<br/>NÃ—CÃ—100Ã—50] --> Norm[æ•°æ®æ ‡å‡†åŒ–]
    Norm --> Strategy{åµŒå…¥ç­–ç•¥}
    
    Strategy -->|3é€šé“| Strategy3[EmbeddingStrategy3Chans]
    Strategy -->|4é€šé“| Strategy4[EmbeddingStrategy4Chans]
    
    Strategy3 --> LambdaProj[LambdaæŠ•å½±] 
    Strategy3 --> CNN3[CNNç¼–ç å™¨]
    
    Strategy4 --> AdaptivePE[è‡ªé€‚åº”Î»ä½ç½®ç¼–ç ]
    Strategy4 --> CNN4[CNN/Linear/MLPç¼–ç å™¨]
    
    LambdaProj --> Combine[lambda_emb + feat_emb]
    CNN3 --> Combine
    AdaptivePE --> Combine
    CNN4 --> Combine
    
    Combine --> PE[ä½ç½®ç¼–ç ]
    PE --> Transformer[Transformerç¼–ç å™¨]
    Transformer --> Norm2[LayerNorm]
    Norm2 --> Projection[æŠ•å½±å¤´]
    Projection --> Output[é¢„æµ‹è¾“å‡º<br/>NÃ—100Ã—3]
```

#### ğŸ”‘ å…³é”®ç»„ä»¶

##### `AdaptiveLambdaEncoding` ç±»
**åˆ›æ–°ç‚¹**: å¯å­¦ä¹ çš„Î»ä½ç½®ç¼–ç 
```python
# ä¼ ç»Ÿä½ç½®ç¼–ç : å›ºå®šé¢‘ç‡
PE[pos] = sin(pos/10000^(2i/d))

# è‡ªé€‚åº”Î»ç¼–ç : å¯å­¦ä¹ é¢‘ç‡
PE[Î»] = sin(Î»*C/10000^(2i/d))  # Cæ˜¯å¯å­¦ä¹ å‚æ•°
```

**ç‰©ç†æ„ä¹‰**: 
- Î»å€¼åœ¨[0,1]èŒƒå›´å†…ï¼Œéœ€è¦ç‰¹æ®Šçš„ç¼–ç ç­–ç•¥
- å¯å­¦ä¹ çš„Cå› å­è®©æ¨¡å‹è‡ªé€‚åº”Î»å€¼åˆ†å¸ƒç‰¹æ€§

##### åµŒå…¥ç­–ç•¥ç³»ç»Ÿ
**è®¾è®¡æ¨¡å¼**: ç­–ç•¥æ¨¡å¼ï¼Œæ ¹æ®è¾“å…¥é€šé“æ•°é€‰æ‹©åµŒå…¥æ–¹å¼

**3é€šé“ç­–ç•¥** (`EmbeddingStrategy3Chans`):
```python
è¾“å…¥: Î¼, ÏƒÂ², error (åˆ†å­åŠ¨åŠ›å­¦åŸºç¡€é‡)
Î»ç‰¹å¾: [Î», Î”Î», window_mask] â†’ LinearæŠ•å½±
æ•°æ®ç‰¹å¾: [Î¼, ÏƒÂ², error] â†’ CNNç¼–ç 
è¾“å‡º: lambda_emb + feat_emb
```

**4é€šé“ç­–ç•¥** (`EmbeddingStrategy4Chans`):
```python
è¾“å…¥: Î¼, ÏƒÂ², error, Î”Î» (åŒ…å«Î”Î»é€šé“)
Î»ç¼–ç : AdaptiveLambdaEncoding(Î»å€¼)
æ•°æ®ç¼–ç : CNN/Linear/MLPä¸‰ç§é€‰æ‹©
è¾“å‡º: lambda_emb + feat_emb
```

##### `MaskedAutoencoderViT` ä¸»æ¨¡å‹
**æ ¸å¿ƒæµç¨‹**:
1. **æ•°æ®æ ‡å‡†åŒ–**: ä½¿ç”¨è®­ç»ƒé›†ç»Ÿè®¡ä¿¡æ¯Z-scoreæ ‡å‡†åŒ–
2. **ç‰¹å¾åµŒå…¥**: æ ¹æ®é€šé“æ•°é€‰æ‹©åµŒå…¥ç­–ç•¥
3. **Transformerç¼–ç **: å¤šå¤´æ³¨æ„åŠ›å­¦ä¹ çª—å£é—´å…³ç³»
4. **ç‰¹å¾æŠ•å½±**: è¾“å‡ºÎ¼ã€Ïƒã€errorä¸‰ä¸ªç‰©ç†é‡

#### ğŸ¯ å¤šæŸå¤±å‡½æ•°ç³»ç»Ÿ

##### 1. æ€»Î”GæŸå¤± (Total Î”G Loss)
```python
# ç‰©ç†å…¬å¼: dG = Î¼ - ÏƒÂ²/2 + error (æ¯çª—å£)
# æ€»Î”G = Î£(dG_i * mask_i) * kbt
pred_total_dg = (pred_dg_per_window * window_mask).sum(dim=1) * 0.592
loss = MSE(pred_total_dg, target_total_dg)
```

##### 2. èšåˆÎ”GæŸå¤± (Aggregation Î”G Loss)
```python
# å¤„ç†ä¸åŒÎ”Î»å€¼çš„çª—å£èšåˆ
# å°†100ä¸ªæ ‡å‡†çª—å£èšåˆå›åŸå§‹çª—å£æ•°é‡
èšåˆæ¯”ä¾‹ = åŸå§‹Î”Î» / 0.01
èšåˆæŸå¤± = dg_aggregation_loss_v2(é¢„æµ‹, åŸå§‹çª—å£ä¿¡æ¯)
```

##### 3. å¹³æ»‘æŸå¤± (Smoothness Loss)
```python
# åŸºäºäºŒé˜¶å¯¼æ•°çš„å¹³æ»‘çº¦æŸ
second_derivative = f[i+2] - 2*f[i+1] + f[i]
smoothness_loss = mean(second_derivativeÂ²)
```

##### 4. ç‰¹å¾æŸå¤± (Feature Loss)
```python
# ç›´æ¥ç›‘ç£Î¼ã€Ïƒã€erroré¢„æµ‹
feature_loss = MSE(pred_mu, target_mu) + 
               MSE(pred_sigma, target_sigma) + 
               MSE(pred_error, target_error)
```

---

### 4. è®­ç»ƒå™¨æ¨¡å— (`trainer.py`)

#### ğŸ¯ æ ¸å¿ƒèŒè´£
å°è£…å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼Œæä¾›é«˜å±‚æ¬¡çš„è®­ç»ƒæ¥å£ã€‚

#### ğŸ”„ è®­ç»ƒæµç¨‹

```mermaid
graph TD
    Setup[setup] --> CreateData[åˆ›å»ºæ•°æ®é›†]
    CreateData --> CalcStats[è®¡ç®—æ•°æ®ç»Ÿè®¡]
    CalcStats --> CreateModel[åˆ›å»ºæ¨¡å‹]
    CreateModel --> CreateOpt[åˆ›å»ºä¼˜åŒ–å™¨]
    CreateOpt --> StartTrain[å¼€å§‹è®­ç»ƒ]
    
    StartTrain --> TrainEpoch[train_one_epoch]
    TrainEpoch --> ValidEpoch[validate_one_epoch]
    ValidEpoch --> SaveModel{ä¿å­˜æ¨¡å‹?}
    SaveModel -->|æ˜¯| SaveBest[ä¿å­˜æœ€ä½³æ¨¡å‹]
    SaveModel -->|å¦| LogMetrics[è®°å½•æŒ‡æ ‡]
    SaveBest --> LogMetrics
    LogMetrics --> NextEpoch{ä¸‹ä¸€è½®?}
    NextEpoch -->|æ˜¯| TrainEpoch
    NextEpoch -->|å¦| Finish[è®­ç»ƒå®Œæˆ]
```

#### ğŸ”‘ å…³é”®ç‰¹æ€§

##### åŠ¨æ€ç»Ÿè®¡è®¡ç®—
```python
# è®­ç»ƒå‰åŠ¨æ€è®¡ç®—æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
for batch in data_loader:
    for channel in [0, 1, 2]:  # Î¼, ÏƒÂ², error
        valid_data = extract_valid_points(batch, channel)
        accumulate_statistics(valid_data)

train_means = sum_features / total_points
train_stds = sqrt(sum_squares/total_points - meansÂ²)
```

##### åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
```python
# è‡ªåŠ¨æ£€æµ‹å¹¶é…ç½®åˆ†å¸ƒå¼è®­ç»ƒ
if world_size > 1:
    sampler = DistributedSampler(dataset)
    model = DistributedDataParallel(model)
else:
    sampler = RandomSampler(dataset)
```

##### æ™ºèƒ½æ£€æŸ¥ç‚¹ç®¡ç†
```python
# åŸºäºéªŒè¯æŸå¤±çš„æœ€ä½³æ¨¡å‹ä¿å­˜
if val_loss < best_val_loss:
    best_val_loss = val_loss
    save_model(model, optimizer, epoch)
```

---

### 5. è®­ç»ƒå¼•æ“æ¨¡å— (`util/enc_engine_*.py`)

#### ğŸ¯ æ ¸å¿ƒèŒè´£
å®ç°å…·ä½“çš„è®­ç»ƒå’ŒéªŒè¯é€»è¾‘ã€‚

#### ğŸ”‘ å…³é”®åŠŸèƒ½

##### `train_one_epoch` (è®­ç»ƒä¸€è½®)
```python
# æ ¸å¿ƒè®­ç»ƒå¾ªç¯
for batch in data_loader:
    # å‰å‘ä¼ æ’­
    loss_dict = model(processed_data, original_data_lists...)
    
    # åå‘ä¼ æ’­
    loss = loss_dict['loss'] / accum_iter
    loss_scaler.scale(loss).backward()
    
    # æ¢¯åº¦ç´¯ç§¯
    if (step + 1) % accum_iter == 0:
        loss_scaler.step(optimizer)
        loss_scaler.update()
        optimizer.zero_grad()
```

##### `validate` (éªŒè¯è¿‡ç¨‹)
```python
# éªŒè¯è¿‡ç¨‹ï¼Œè®¡ç®—å¤šç§æŒ‡æ ‡
metrics = {
    'loss': å¹³å‡æŸå¤±,
    'total_dg_mae': æ€»Î”Gå¹³å‡ç»å¯¹è¯¯å·®,
    'feature_mae': ç‰¹å¾å¹³å‡ç»å¯¹è¯¯å·®,
    'predictions': é¢„æµ‹ç»“æœ
}

# ä¿å­˜è¯¦ç»†éªŒè¯ç»“æœ
save_validation_results(predictions, targets, 'validation_results.csv')
save_per_window_results(window_predictions, 'per_window_results.csv')
```

---

### 6. æŸå¤±è®¡ç®—æ¨¡å— (`util/enc_model_dg_loss.py`)

#### ğŸ¯ æ ¸å¿ƒèŒè´£
å®ç°å¤æ‚çš„Î”GèšåˆæŸå¤±è®¡ç®—ã€‚

#### ğŸ”‘ å…³é”®ç®—æ³•

##### `dg_aggregation_loss_v2`
**é—®é¢˜**: æ¨¡å‹é¢„æµ‹100ä¸ªæ ‡å‡†çª—å£ï¼Œä½†åŸå§‹æ•°æ®æœ‰ä¸åŒæ•°é‡å’Œå¤§å°çš„çª—å£
**è§£å†³**: æ™ºèƒ½èšåˆç®—æ³•

```python
# èšåˆç®—æ³•æ ¸å¿ƒé€»è¾‘
for åŸå§‹çª—å£ in åŸå§‹çª—å£åˆ—è¡¨:
    èšåˆæ¯”ä¾‹ = åŸå§‹çª—å£.Î”Î» / 0.01
    èµ·å§‹ç´¢å¼• = round(åŸå§‹çª—å£.Î» / 0.01)
    ç»“æŸç´¢å¼• = èµ·å§‹ç´¢å¼• + èšåˆæ¯”ä¾‹
    
    èšåˆé¢„æµ‹Î”G = sum(pred_dg[èµ·å§‹:ç»“æŸ]) * kbt
    loss += (èšåˆé¢„æµ‹Î”G - åŸå§‹Î”G)Â²
```

**ç‰©ç†æ„ä¹‰**:
- ä¿æŒèƒ½é‡å®ˆæ’
- å¤„ç†ä¸åŒåˆ†è¾¨ç‡çš„çª—å£
- å•ä½è½¬æ¢ (æ— é‡çº² â†’ kcal/mol)

---

## ğŸŒŠ å®Œæ•´æ•°æ®æµåˆ†æ

### æ•°æ®æµå‘å›¾

```mermaid
graph TD
    subgraph "åŸå§‹æ•°æ®"
        CSV1[system1_lambda0.1_delta0.05.csv]
        CSV2[system1_lambda0.15_delta0.05.csv]
        Target[free_ene_zwanzig.csv]
    end
    
    subgraph "æ•°æ®é¢„å¤„ç†"
        Parse[æ–‡ä»¶åè§£æÎ»,Î”Î»]
        Sample[éšæœºå­é›†é‡‡æ ·]
        Process[LambdaDataProcessorå¯¹é½]
    end
    
    subgraph "æ‰¹å¤„ç†"
        Collate[custom_collate_fn]
        Batch[æ‰¹æ¬¡å¼ é‡ç»„ç»‡]
    end
    
    subgraph "æ¨¡å‹æ¨ç†"
        Normalize[æ•°æ®æ ‡å‡†åŒ–]
        Embed[ç‰¹å¾åµŒå…¥]
        Transform[Transformerç¼–ç ]
        Project[æŠ•å½±è¾“å‡º]
    end
    
    subgraph "æŸå¤±è®¡ç®—"
        Denorm[åæ ‡å‡†åŒ–]
        CalcDG[è®¡ç®—Î”G]
        MultiLoss[å¤šæŸå¤±å‡½æ•°]
    end
    
    CSV1 --> Parse
    CSV2 --> Parse
    Target --> Sample
    Parse --> Sample
    Sample --> Process
    Process --> Collate
    Collate --> Batch
    
    Batch --> Normalize
    Normalize --> Embed
    Embed --> Transform
    Transform --> Project
    
    Project --> Denorm
    Denorm --> CalcDG
    CalcDG --> MultiLoss
```

### å…³é”®æ•°æ®å˜æ¢

#### 1. æ–‡ä»¶ â†’ æ•°æ®é›†
```python
# è¾“å…¥: CSVæ–‡ä»¶
"system1_lambda0.1_delta0.05.csv" â†’ 
{
    'data': [Î¼æ—¶é—´åºåˆ—, ÏƒÂ²æ—¶é—´åºåˆ—, erroræ—¶é—´åºåˆ—],
    'lambda': 0.1,
    'delta': 0.05,
    'length': å®é™…æ•°æ®ç‚¹æ•°
}
```

#### 2. æ•°æ®é›† â†’ æ ‡å‡†ç½‘æ ¼
```python
# LambdaDataProcessorå¤„ç†
å¯å˜é•¿åº¦çª—å£ â†’ 100çª—å£æ ‡å‡†ç½‘æ ¼
data: [N_samples, variable_windows, variable_length, 3] â†’ 
      [N, 3, 100, 50] + masks + lengths
```

#### 3. æ¨¡å‹è¾“å…¥ â†’ è¾“å‡º
```python
# æ¨¡å‹æ¨ç†
è¾“å…¥: [N, C, 100, 50] (æ ‡å‡†åŒ–å)
â†“ åµŒå…¥ç­–ç•¥
[N, 100, embed_dim] (lambda_emb + feat_emb)
â†“ Transformer
[N, 100, embed_dim] (ç¼–ç ç‰¹å¾)
â†“ æŠ•å½±å¤´
è¾“å‡º: [N, 100, 3] (Î¼, Ïƒ, erroré¢„æµ‹)
```

#### 4. é¢„æµ‹ â†’ æŸå¤±
```python
# æŸå¤±è®¡ç®—
é¢„æµ‹: [N, 100, 3] â†’ åæ ‡å‡†åŒ– â†’ [N, 100, 3]
â†“ è®¡ç®—Î”G
æ¯çª—å£Î”G: [N, 100] = Î¼ - ÏƒÂ²/2 + error
â†“ å¤šæŸå¤±å‡½æ•°
æ€»æŸå¤± = w1*æ€»Î”GæŸå¤± + w2*èšåˆæŸå¤± + w3*å¹³æ»‘æŸå¤± + w4*ç‰¹å¾æŸå¤±
```

---

## ğŸ”¬ æ ¸å¿ƒç®—æ³•æ·±åº¦è§£æ

### 1. è‡ªé€‚åº”Î»ä½ç½®ç¼–ç ç®—æ³•

#### ä¼ ç»Ÿä½ç½®ç¼–ç é—®é¢˜
- æ ‡å‡†Transformerä½ç½®ç¼–ç é’ˆå¯¹è‡ªç„¶è¯­è¨€åºåˆ—ä½ç½®
- Î»å€¼æ˜¯ç‰©ç†é‡ï¼ŒèŒƒå›´[0,1]ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
- ä¸åŒÎ»å€¼ä¹‹é—´çš„"è·ç¦»"æœ‰ç‰©ç†æ„ä¹‰

#### åˆ›æ–°è§£å†³æ–¹æ¡ˆ
```python
class AdaptiveLambdaEncoding(nn.Module):
    def __init__(self, d_model, init_C=100.0):
        # Cæ˜¯å¯å­¦ä¹ å‚æ•°ï¼Œè°ƒèŠ‚ç¼–ç é¢‘ç‡
        self.C = nn.Parameter(torch.tensor(init_C))
        
    def forward(self, lambda_val):
        # Î»å€¼ç¼©æ”¾
        lambda_scaled = lambda_val * self.C
        
        # æ­£å¼¦ä½™å¼¦ç¼–ç 
        pe[..., 0::2] = sin(lambda_scaled * div_term)
        pe[..., 1::2] = cos(lambda_scaled * div_term)
        
        return pe
```

#### ç‰©ç†æ„ä¹‰
- **Cå› å­å­¦ä¹ **: æ¨¡å‹è‡ªåŠ¨å­¦ä¹ æœ€é€‚åˆÎ»å€¼åˆ†å¸ƒçš„ç¼–ç é¢‘ç‡
- **è¿ç»­æ€§ä¿è¯**: Î»å€¼ç›¸è¿‘çš„çª—å£è·å¾—ç›¸ä¼¼çš„ç¼–ç 
- **å¯åŒºåˆ†æ€§**: ä¸åŒÎ»å€¼è·å¾—å……åˆ†åŒºåˆ†çš„ç¼–ç 

### 2. å¤šæŸå¤±å‡½æ•°ååŒä¼˜åŒ–

#### æŸå¤±å‡½æ•°è®¾è®¡ç†å¿µ
```python
# ç‰©ç†çº¦æŸ + æ•°æ®æ‹Ÿåˆ + å¹³æ»‘çº¦æŸ
æ€»æŸå¤± = ç‰©ç†ä¸€è‡´æ€§æŸå¤± + æ•°æ®é‡å»ºæŸå¤± + æ­£åˆ™åŒ–æŸå¤±
```

#### å…·ä½“å®ç°
```python
def forward_loss(self, pred, targets):
    loss_dict = {}
    total_loss = 0
    
    # 1. ç‰©ç†ä¸€è‡´æ€§ - æ€»Î”GæŸå¤±
    if self.total_dg_loss_weight > 0:
        pred_total_dg = compute_total_dg(pred)
        target_total_dg = get_target_total_dg(targets)
        total_dg_loss = F.mse_loss(pred_total_dg, target_total_dg)
        total_loss += self.total_dg_loss_weight * total_dg_loss
    
    # 2. å°ºåº¦ä¸€è‡´æ€§ - èšåˆæŸå¤±  
    if self.agg_dg_loss_weight > 0:
        agg_loss = dg_aggregation_loss_v2(pred, original_windows)
        total_loss += self.agg_dg_loss_weight * agg_loss
    
    # 3. ç‰©ç†åˆç†æ€§ - å¹³æ»‘æŸå¤±
    if self.smoothness_loss_weight > 0:
        smoothness_loss = compute_smoothness_loss(pred)
        total_loss += self.smoothness_loss_weight * smoothness_loss
    
    # 4. æ•°æ®æ‹Ÿåˆ - ç‰¹å¾æŸå¤±
    if self.feature_loss_weight > 0:
        feature_loss = compute_feature_loss(pred, targets)
        total_loss += self.feature_loss_weight * feature_loss
    
    return {'loss': total_loss, **loss_dict}
```

### 3. çª—å£èšåˆç®—æ³•

#### é—®é¢˜æè¿°
- **æ¨¡å‹è¾“å‡º**: 100ä¸ªæ ‡å‡†0.01é—´éš”çª—å£çš„Î”Gé¢„æµ‹
- **çœŸå®æ•°æ®**: ä»»æ„æ•°é‡ã€ä»»æ„Î”Î»å¤§å°çš„çª—å£
- **ç›®æ ‡**: å°†æ ‡å‡†çª—å£èšåˆåˆ°åŸå§‹çª—å£è¿›è¡Œæ¯”è¾ƒ

#### èšåˆç®—æ³•
```python
def dg_aggregation_loss_v2(pred_dg_per_window, window_mask, 
                          original_lambdas, original_deltas, original_dGs):
    """
    æ™ºèƒ½çª—å£èšåˆç®—æ³•
    """
    loss = 0
    for sample_idx in range(batch_size):
        sample_pred = pred_dg_per_window[sample_idx]  # [100]
        sample_mask = window_mask[sample_idx]         # [100]
        
        for orig_window_idx, (lambda_val, delta_val, target_dg) in enumerate(
            zip(original_lambdas[sample_idx], 
                original_deltas[sample_idx], 
                original_dGs[sample_idx])):
            
            # è®¡ç®—èšåˆèŒƒå›´
            start_idx = round(lambda_val / 0.01)
            aggregation_ratio = delta_val / 0.01
            end_idx = start_idx + aggregation_ratio
            
            # èšåˆé¢„æµ‹Î”G
            if end_idx <= 100:
                mask_slice = sample_mask[start_idx:end_idx]
                pred_slice = sample_pred[start_idx:end_idx]
                
                if mask_slice.sum() > 0:
                    # åŠ æƒèšåˆ + å•ä½è½¬æ¢
                    aggregated_pred_dg = (pred_slice * mask_slice).sum() * kbt
                    target_dg_with_unit = target_dg * kbt
                    
                    # ç´¯ç§¯æŸå¤±
                    loss += (aggregated_pred_dg - target_dg_with_unit) ** 2
    
    return loss / batch_size
```

#### ç®—æ³•åˆ›æ–°ç‚¹
- **å°ºåº¦é€‚åº”**: è‡ªåŠ¨å¤„ç†ä¸åŒÎ”Î»å¤§å°çš„çª—å£
- **æ©ç ä¿æŠ¤**: åªèšåˆæœ‰æ•ˆçª—å£ï¼Œé¿å…å¡«å……æ•°æ®å¹²æ‰°
- **å•ä½ç»Ÿä¸€**: è‡ªåŠ¨è¿›è¡Œkbtå•ä½è½¬æ¢
- **èƒ½é‡å®ˆæ’**: ä¿è¯èšåˆè¿‡ç¨‹çš„ç‰©ç†ä¸€è‡´æ€§

---

## ğŸš€ è¿è¡ŒæŒ‡å—

### å¿«é€Ÿå¼€å§‹
```bash
# åŸºç¡€è®­ç»ƒ
python main_train.py \
    --data_path /path/to/data \
    --epochs 100 \
    --batch_size 4

# å®Œæ•´é…ç½®è®­ç»ƒ
python main_train.py \
    --data_path /path/to/data \
    --output_dir ./outputs \
    --batch_size 4 \
    --epochs 100 \
    --model enc_cnn_chans3 \
    --total_dg_loss_weight 1.0 \
    --agg_dg_loss_weight 1.0 \
    --smoothness_loss_weight 0.1 \
    --feature_loss_weight 1.0
```

### æ•°æ®æ ¼å¼è¦æ±‚
```
data_path/
â”œâ”€â”€ train/
â”‚   â””â”€â”€ system_X/
â”‚       â”œâ”€â”€ complex/
â”‚       â”‚   â”œâ”€â”€ data_lambda0.05_delta0.01.csv    # Î¼,ÏƒÂ²,erroræ—¶é—´åºåˆ—
â”‚       â”‚   â”œâ”€â”€ data_lambda0.06_delta0.01.csv
â”‚       â”‚   â””â”€â”€ fe_cal_out/
â”‚       â”‚       â””â”€â”€ free_ene_zwanzig.csv         # ç›®æ ‡Î”Gå€¼
â”‚       â””â”€â”€ ligand/
â”‚           â””â”€â”€ (ç›¸åŒç»“æ„)
â””â”€â”€ val/
    â””â”€â”€ (ç›¸åŒç»“æ„)
```

### æ¨¡å‹é€‰æ‹©æŒ‡å—

#### 3é€šé“æ¨¡å‹ (enc_cnn_chans3)
- **é€‚ç”¨**: åŸºç¡€åˆ†å­åŠ¨åŠ›å­¦æ•°æ®
- **è¾“å…¥**: Î¼, ÏƒÂ², error
- **ç‰¹ç‚¹**: LambdaæŠ•å½± + CNNç¼–ç 
- **æ¨è**: æ ‡å‡†è‡ªç”±èƒ½è®¡ç®—ä»»åŠ¡

#### 4é€šé“æ¨¡å‹ (enc_cnn_chans4)  
- **é€‚ç”¨**: åŒ…å«Î”Î»ä¿¡æ¯çš„æ•°æ®
- **è¾“å…¥**: Î¼, ÏƒÂ², error, Î”Î»
- **ç‰¹ç‚¹**: è‡ªé€‚åº”Î»ç¼–ç  + å¤šç§ç¼–ç æ–¹å¼
- **æ¨è**: å¤æ‚çª—å£ç»“æ„æ•°æ®

---

## ğŸ” è°ƒè¯•å’Œæ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. å†…å­˜ä¸è¶³
**ç—‡çŠ¶**: CUDA out of memory
**è§£å†³**: 
- å‡å°‘`batch_size`
- å‡å°‘`per_lambda_max_points`
- æ£€æŸ¥`num_random_subsets_per_system`

#### 2. æ”¶æ•›å›°éš¾
**ç—‡çŠ¶**: æŸå¤±ä¸ä¸‹é™æˆ–éœ‡è¡
**è§£å†³**:
- è°ƒæ•´æŸå¤±æƒé‡å¹³è¡¡
- é™ä½å­¦ä¹ ç‡
- å¢åŠ warmupè½®æ•°
- æ£€æŸ¥æ•°æ®æ ‡å‡†åŒ–

#### 3. æ•°æ®æ ¼å¼é”™è¯¯
**ç—‡çŠ¶**: æ•°æ®åŠ è½½å¤±è´¥
**è§£å†³**:
- æ£€æŸ¥CSVæ–‡ä»¶æ ¼å¼
- éªŒè¯æ–‡ä»¶åÎ»,Î”Î»è§£æ
- ç¡®è®¤free_ene.csvå­˜åœ¨

### è°ƒè¯•æŠ€å·§

#### å¯ç”¨è¯¦ç»†æ—¥å¿—
```python
# åœ¨trainer.pyä¸­æ·»åŠ 
import logging
logging.basicConfig(level=logging.DEBUG)
```

#### å¯è§†åŒ–æ•°æ®æµ
```python
# åœ¨æ¨¡å‹ä¸­æ·»åŠ ä¸­é—´è¾“å‡º
def forward_encoder(self, ...):
    x = self.embedding_module(...)
    print(f"åµŒå…¥è¾“å‡ºå½¢çŠ¶: {x.shape}")
    
    x = x + self.pos_embed
    print(f"ä½ç½®ç¼–ç åå½¢çŠ¶: {x.shape}")
    
    for i, blk in enumerate(self.blocks):
        x = blk(x)
        if i < 3:  # åªæ‰“å°å‰å‡ å±‚
            print(f"ç¬¬{i}å±‚Transformerè¾“å‡º: {x.shape}")
```

#### æŸå¤±åˆ†æ
```python
# ç›‘æ§å„æŸå¤±åˆ†é‡
def forward_loss(self, ...):
    loss_dict = {...}
    
    # æ‰“å°æŸå¤±æƒé‡å’Œæ•°å€¼
    for key, value in loss_dict.items():
        if 'loss' in key:
            print(f"{key}: {value.item():.6f}")
    
    return loss_dict
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### è®­ç»ƒåŠ é€Ÿ
1. **æ··åˆç²¾åº¦è®­ç»ƒ**: ä½¿ç”¨AMPè‡ªåŠ¨æ··åˆç²¾åº¦
2. **æ¢¯åº¦ç´¯ç§¯**: å¢åŠ æœ‰æ•ˆæ‰¹æ¬¡å¤§å°
3. **æ•°æ®é¢„å–**: å¢åŠ `num_workers`
4. **å†…å­˜å›ºå®š**: å¯ç”¨`pin_memory`

### å†…å­˜ä¼˜åŒ–
1. **æ‰¹æ¬¡å¤§å°**: æ ¹æ®GPUå†…å­˜è°ƒæ•´
2. **æ•°æ®ç‚¹é‡‡æ ·**: é™åˆ¶`per_lambda_max_points`
3. **å­é›†æ•°é‡**: è°ƒæ•´`num_random_subsets_per_system`
4. **æ£€æŸ¥ç‚¹**: å®šæœŸæ¸…ç†ä¸­é—´ç»“æœ

### æ¨¡å‹ä¼˜åŒ–
1. **åµŒå…¥ç»´åº¦**: æ ¹æ®æ•°æ®å¤æ‚åº¦è°ƒæ•´`embed_dim`
2. **Transformerå±‚æ•°**: å¹³è¡¡æ·±åº¦å’Œè®¡ç®—æˆæœ¬
3. **æŸå¤±æƒé‡**: æ ¹æ®ä»»åŠ¡é‡è¦æ€§è°ƒæ•´æƒé‡

---

## ğŸ”¬ æ‰©å±•æŒ‡å—

### æ·»åŠ æ–°çš„åµŒå…¥ç­–ç•¥
```python
class MyEmbeddingStrategy(BaseEmbeddingModule):
    def __init__(self, embed_dim, img_size, in_chans, **kwargs):
        super().__init__()
        self.my_encoder = create_my_encoder(...)
    
    def forward(self, x, lambdas, deltas, masks, original_lengths):
        # å®ç°ä½ çš„åµŒå…¥é€»è¾‘
        lambda_emb = self.lambda_encoding(lambdas)
        feat_emb = self.my_encoder(x)
        return lambda_emb, feat_emb

# åœ¨MaskedAutoencoderViTä¸­æ³¨å†Œ
if in_chans == 5:  # æ–°çš„é€šé“æ•°
    self.embedding_module = MyEmbeddingStrategy(...)
```

### æ·»åŠ æ–°çš„æŸå¤±å‡½æ•°
```python
def forward_loss(self, pred, ...):
    # ç°æœ‰æŸå¤±è®¡ç®—...
    
    # æ·»åŠ æ–°æŸå¤±
    if self.my_loss_weight > 0:
        my_loss = self.compute_my_loss(pred, targets)
        loss_dict['my_loss'] = my_loss
        total_loss += self.my_loss_weight * my_loss
    
    return loss_dict

def compute_my_loss(self, pred, targets):
    # å®ç°ä½ çš„æŸå¤±é€»è¾‘
    return loss_value
```

### è‡ªå®šä¹‰æ•°æ®é¢„å¤„ç†
```python
class MyDataProcessor(LambdaDataProcessor):
    def process(self, original_data, ...):
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        result = super().process(original_data, ...)
        
        # æ·»åŠ ä½ çš„å¤„ç†é€»è¾‘
        result['my_feature'] = self.compute_my_feature(original_data)
        
        return result
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

### æ ¸å¿ƒè®ºæ–‡
- **MAE**: "Masked Autoencoders Are Scalable Vision Learners"
- **Vision Transformer**: "An Image is Worth 16x16 Words"
- **è‡ªç”±èƒ½è®¡ç®—**: åˆ†å­åŠ¨åŠ›å­¦ç›¸å…³è®ºæ–‡

### æŠ€æœ¯æ–‡æ¡£
- **PyTorchå®˜æ–¹æ–‡æ¡£**: https://pytorch.org/docs/
- **timmåº“æ–‡æ¡£**: https://timm.fast.ai/
- **TensorBoardä½¿ç”¨æŒ‡å—**: https://www.tensorflow.org/tensorboard

### ä»£ç é£æ ¼
- **PEP 8**: Pythonä»£ç é£æ ¼æŒ‡å—
- **Google Python Style**: è¯¦ç»†çš„Pythonç¼–ç¨‹è§„èŒƒ
- **Type Hints**: Pythonç±»å‹æç¤ºæœ€ä½³å®è·µ

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

### ä»£ç è´¡çŒ®æµç¨‹
1. Forké¡¹ç›®åˆ°ä½ çš„GitHub
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯: `git checkout -b feature/amazing-feature`
3. ç¼–å†™ä»£ç å¹¶ç¡®ä¿é€šè¿‡æµ‹è¯•
4. æäº¤æ›´æ”¹: `git commit -m 'Add amazing feature'`
5. æ¨é€åˆ†æ”¯: `git push origin feature/amazing-feature`
6. åˆ›å»ºPull Request

### ä»£ç è´¨é‡è¦æ±‚
- **ä»£ç é£æ ¼**: éµå¾ªPEP 8è§„èŒƒ
- **ç±»å‹æç¤º**: ä¸ºæ‰€æœ‰å‡½æ•°æ·»åŠ ç±»å‹æç¤º
- **æ–‡æ¡£å­—ç¬¦ä¸²**: è¯¦ç»†çš„ä¸­æ–‡docstring
- **å•å…ƒæµ‹è¯•**: ä¸ºæ–°åŠŸèƒ½æ·»åŠ æµ‹è¯•
- **æ€§èƒ½æµ‹è¯•**: ç¡®ä¿ä¸é™ä½è®­ç»ƒæ€§èƒ½

### æäº¤ä¿¡æ¯æ ¼å¼
```
ğŸš€ feat: æ·»åŠ æ–°çš„åµŒå…¥ç­–ç•¥

- å®ç°MyEmbeddingStrategyç±»
- æ”¯æŒ5é€šé“è¾“å…¥æ•°æ®
- æ·»åŠ ç›¸åº”çš„é…ç½®é€‰é¡¹
- æ›´æ–°æ–‡æ¡£å’Œæµ‹è¯•

Closes #123
```

---

> ğŸ’¡ **æ€»ç»“**: è¿™ä¸ªé¡¹ç›®å±•ç°äº†ä¼˜ç§€çš„è½¯ä»¶å·¥ç¨‹å®è·µï¼Œå°†å¤æ‚çš„åˆ†å­åŠ¨åŠ›å­¦é—®é¢˜è½¬åŒ–ä¸ºæ·±åº¦å­¦ä¹ é—®é¢˜ï¼Œå¹¶é€šè¿‡åˆ›æ–°çš„æ¨¡å‹è®¾è®¡å’ŒæŸå¤±å‡½æ•°æ¥è§£å†³ç‰¹å®šé¢†åŸŸçš„æŒ‘æˆ˜ã€‚ä»£ç ç»“æ„æ¸…æ™°ï¼Œæ¨¡å—åŒ–ç¨‹åº¦é«˜ï¼Œä¸ºç§‘ç ”å’Œå·¥ç¨‹å®è·µæä¾›äº†ä¼˜ç§€çš„èŒƒä¾‹ã€‚

---

ğŸ“… **æœ€åæ›´æ–°**: 2024å¹´6æœˆ  
ğŸ“§ **æŠ€æœ¯æ”¯æŒ**: é€šè¿‡GitHub Issuesæå‡ºé—®é¢˜  
ğŸŒŸ **é¡¹ç›®åœ°å€**: https://github.com/phloglucinol/enc-cnn-refactored